{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load api keys\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create necessary directories\n",
    "input_image_path = Path(\"input_images\")\n",
    "data_path = Path(\"mixed_wiki\")\n",
    "Path.mkdir(data_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "image_paths = []\n",
    "for img_path in os.listdir(\"./input_images\"):\n",
    "    image_paths.append(str(os.path.join(\"./input_images\", img_path)))\n",
    "\n",
    "\n",
    "def plot_images(image_paths):\n",
    "    images_shown = 0\n",
    "    plt.figure(figsize=(16, 9))\n",
    "    for img_path in image_paths:\n",
    "        if os.path.isfile(img_path):\n",
    "            image = Image.open(img_path)\n",
    "\n",
    "            plt.subplot(2, 3, images_shown + 1)\n",
    "            plt.imshow(image)\n",
    "            plt.xticks([])\n",
    "            plt.yticks([])\n",
    "\n",
    "            images_shown += 1\n",
    "            if images_shown >= 9:\n",
    "                break\n",
    "\n",
    "\n",
    "plot_images(image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text descriptions for images using GPT-4o\n",
    "# this is just a sample how to get description for images, we do not use this in our rag model\n",
    "from llama_index.multi_modal_llms.openai import OpenAIMultiModal\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "image_documents = SimpleDirectoryReader(\"./input_images\").load_data()\n",
    "openai_mm_llm = OpenAIMultiModal(model=\"gpt-4o\", api_key=os.getenv(\"OPENAI_API_KEY\"), max_new_tokens=1500)\n",
    "\n",
    "response_1 = openai_mm_llm.complete(prompt=\"Generate a text description for each image\", image_documents=image_documents)\n",
    "print(response_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wikipedia_images(title):\n",
    "    response = requests.get(\n",
    "        \"https://en.wikipedia.org/w/api.php\",\n",
    "        params={\n",
    "            \"action\": \"query\",\n",
    "            \"format\": \"json\",\n",
    "            \"titles\": title,\n",
    "            \"prop\": \"imageinfo\",\n",
    "            \"iiprop\": \"url|dimensions|mime\",\n",
    "            \"generator\": \"images\",\n",
    "            \"gimlimit\": \"50\",\n",
    "        },\n",
    "    ).json()\n",
    "    image_urls = []\n",
    "    for page in response[\"query\"][\"pages\"].values():\n",
    "        if page[\"imageinfo\"][0][\"url\"].endswith((\".jpg\", \".png\")):\n",
    "            image_urls.append(page[\"imageinfo\"][0][\"url\"])\n",
    "    return image_urls\n",
    "\n",
    "# List of Wikipedia titles to fetch\n",
    "wiki_titles = {\n",
    "    \"Tesla Model Y\",\n",
    "    \"Tesla Model X\",\n",
    "    \"Tesla Model 3\",\n",
    "    \"Tesla Model S\",\n",
    "    \"Kia EV6\",\n",
    "    \"BMW i3\",\n",
    "    \"Audi e-tron\",\n",
    "    \"Ford Mustang\",\n",
    "    \"Porsche Taycan\",\n",
    "    \"Rivian\",\n",
    "    \"Polestar\",\n",
    "}\n",
    "\n",
    "# Fetch text and images\n",
    "import urllib\n",
    "import time\n",
    "\n",
    "image_uuid = 0\n",
    "MAX_IMAGES_PER_WIKI = 5\n",
    "\n",
    "for title in wiki_titles:\n",
    "    # Fetch text\n",
    "    response = requests.get(\n",
    "        \"https://en.wikipedia.org/w/api.php\",\n",
    "        params={\n",
    "            \"action\": \"query\",\n",
    "            \"format\": \"json\",\n",
    "            \"titles\": title,\n",
    "            \"prop\": \"extracts\",\n",
    "            \"explaintext\": True,\n",
    "        },\n",
    "    ).json()\n",
    "    page = next(iter(response[\"query\"][\"pages\"].values()))\n",
    "    wiki_text = page[\"extract\"]\n",
    "\n",
    "    with open(data_path / f\"{title}.txt\", \"w\", encoding=\"utf-8\") as fp:\n",
    "        fp.write(wiki_text)\n",
    "\n",
    "    # Fetch images\n",
    "    images_per_wiki = 0\n",
    "    list_img_urls = get_wikipedia_images(title)\n",
    "\n",
    "    for url in list_img_urls:\n",
    "        if url.lower().endswith((\".jpg\", \".png\", \".svg\")):\n",
    "            image_uuid += 1\n",
    "            print(f\"Downloading image {url} from {title}\")\n",
    "            urllib.request.urlretrieve(\n",
    "                    url, data_path / f\"{image_uuid}.jpg\"\n",
    "                )\n",
    "            time.sleep(1)\n",
    "            images_per_wiki += 1\n",
    "            if images_per_wiki >= MAX_IMAGES_PER_WIKI:\n",
    "                break\n",
    "\n",
    "print(\"Data collection completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the multimodal index\n",
    "from llama_index.core.indices import MultiModalVectorStoreIndex\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "from llama_index.core import SimpleDirectoryReader, StorageContext\n",
    "import qdrant_client\n",
    "\n",
    "# Create a local Qdrant vector store\n",
    "client = qdrant_client.QdrantClient(path=\"qdrant_mm_db\")\n",
    "\n",
    "text_store = QdrantVectorStore(client=client, collection_name=\"text_collection\")\n",
    "image_store = QdrantVectorStore(client=client, collection_name=\"image_collection\")\n",
    "\n",
    "storage_context = StorageContext.from_defaults(\n",
    "    vector_store=text_store, image_store=image_store\n",
    ")\n",
    "\n",
    "# Create the MultiModal index\n",
    "documents = SimpleDirectoryReader(\"./mixed_wiki/\").load_data()\n",
    "index = MultiModalVectorStoreIndex.from_documents(\n",
    "    documents,\n",
    "    storage_context=storage_context,\n",
    ")\n",
    "\n",
    "print(\"Multimodal index built successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement multimodal retrieval\n",
    "from llama_index.core.response.notebook_utils import display_source_node\n",
    "from llama_index.core.schema import ImageNode\n",
    "\n",
    "MAX_TOKENS = 50\n",
    "retriever_engine = index.as_retriever(\n",
    "    similarity_top_k=3, image_similarity_top_k=3\n",
    ")\n",
    "\n",
    "def retrieve_and_display(query):\n",
    "    retrieval_results = retriever_engine.retrieve(query[:MAX_TOKENS])\n",
    "\n",
    "    retrieved_images = []\n",
    "    for res_node in retrieval_results:\n",
    "        if isinstance(res_node.node, ImageNode):\n",
    "            retrieved_images.append(res_node.node.metadata[\"file_path\"])\n",
    "        else:\n",
    "            display_source_node(res_node, source_length=200)\n",
    "\n",
    "    if retrieved_images:\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        for i, img_path in enumerate(retrieved_images):\n",
    "            plt.subplot(1, len(retrieved_images), i+1)\n",
    "            img = Image.open(img_path)\n",
    "            plt.imshow(img)\n",
    "            plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "# Example usage\n",
    "retrieve_and_display(\"What is the best electric Sedan?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multimodal rag query\n",
    "from llama_index.core import PromptTemplate\n",
    "from llama_index.core.query_engine import SimpleMultiModalQueryEngine\n",
    "\n",
    "qa_tmpl_str = (\n",
    "    \"Context information is below.\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"Given the context information and not prior knowledge, \"\n",
    "    \"answer the query.\\n\"\n",
    "    \"Query: {query_str}\\n\"\n",
    "    \"Answer: \"\n",
    ")\n",
    "qa_tmpl = PromptTemplate(qa_tmpl_str)\n",
    "\n",
    "query_engine = index.as_query_engine(\n",
    "    llm=openai_mm_llm, text_qa_template=qa_tmpl\n",
    ")\n",
    "\n",
    "def multimodal_rag_query(query_str):\n",
    "    response = query_engine.query(query_str)\n",
    "    print(\"Answer:\", str(response))\n",
    "\n",
    "    print(\"\\nSources:\")\n",
    "    for text_node in response.metadata[\"text_nodes\"]:\n",
    "        display_source_node(text_node, source_length=200)\n",
    "\n",
    "    if response.metadata[\"image_nodes\"]:\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        for i, img_node in enumerate(response.metadata[\"image_nodes\"]):\n",
    "            plt.subplot(1, len(response.metadata[\"image_nodes\"]), i+1)\n",
    "            img = Image.open(img_node.metadata[\"file_path\"])\n",
    "            plt.imshow(img)\n",
    "            plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "# Example usage\n",
    "multimodal_rag_query(\"Compare the design features of Tesla Model S and Rivian R1\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
