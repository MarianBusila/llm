{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "wiki_titles = [\"RoboCop\", \"Labour Party (UK)\", \"SpaceX\", \"OpenAI\"]\n",
    "\n",
    "data_path = Path(\"data_wiki\")\n",
    "data_path.mkdir(exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download data text from Wikipedia\n",
    "import requests\n",
    "import io\n",
    "\n",
    "for title in wiki_titles:\n",
    "    response = requests.get(\"https://en.wikipedia.org/w/api.php\", params={\n",
    "        \"action\": \"query\",\n",
    "        \"format\" : \"json\",\n",
    "        \"titles\" : title,\n",
    "        \"prop\": \"extracts\",\n",
    "        \"explainText\": True\n",
    "    }\n",
    "    ).json()\n",
    "    page = next(iter(response[\"query\"][\"pages\"].values()))\n",
    "    extract = page[\"extract\"]\n",
    "    with io.open(data_path / f\"{title}.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(extract)\n",
    "    print(f\"Downloaded {title}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download images from Wikipedia\n",
    "import wikipedia\n",
    "import urllib.request\n",
    "\n",
    "image_uuid = 0\n",
    "# image_metadata_dict stores images metadata including image uuid, filename and path\n",
    "image_metadata_dict = {}\n",
    "MAX_IMAGES_PER_WIKI = 15\n",
    "\n",
    "# Download images for wiki pages\n",
    "# Assing UUID for each image\n",
    "for title in wiki_titles:\n",
    "    images_per_wiki = 0\n",
    "    print(title)\n",
    "    try:    \n",
    "        page_py = wikipedia.page(title)\n",
    "        list_img_urls = page_py.images\n",
    "        for url in list_img_urls:\n",
    "            if url.lower().endswith(('.jpg', '.png')):\n",
    "                image_uuid += 1\n",
    "                image_file_name = title + \"_\" + url.split(\"/\")[-1]\n",
    "\n",
    "                # img_path could be s3 path pointing to the raw image file in the future\n",
    "                image_metadata_dict[image_uuid] = {\n",
    "                    \"filename\": image_file_name,\n",
    "                    \"img_path\": \"./\" + str(data_path / f\"{image_uuid}.jpg\"),\n",
    "                }\n",
    "                print(f\"Downloading image from {url}\") \n",
    "                urllib.request.urlretrieve(\n",
    "                    url, data_path / f\"{image_uuid}.jpg\"\n",
    "                )\n",
    "                images_per_wiki += 1\n",
    "                # Limit the number of images downloaded per wiki page to 15\n",
    "                if images_per_wiki > MAX_IMAGES_PER_WIKI:\n",
    "                    break\n",
    "    except:\n",
    "        print(str(Exception(\"No images found for Wikipedia page: \")) + title)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load api keys\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build multi-modal vector store using text and image embeddings under different collections\n",
    "import qdrant_client\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "from llama_index.core import VectorStoreIndex, StorageContext\n",
    "from llama_index.core.indices import MultiModalVectorStoreIndex\n",
    "\n",
    "# Create a local Qdrant vector store\n",
    "client = qdrant_client.QdrantClient(path=\"qdrant_d_0\")\n",
    "\n",
    "text_store = QdrantVectorStore(\n",
    "    client=client, collection_name=\"text_collection_0\"\n",
    ")\n",
    "image_store = QdrantVectorStore(\n",
    "    client=client, collection_name=\"image_collection_0\"\n",
    ")\n",
    "storage_context = StorageContext.from_defaults(\n",
    "    vector_store=text_store, image_store=image_store\n",
    ")\n",
    "\n",
    "# Create the MultiModal index\n",
    "documents = SimpleDirectoryReader(\"./data_wiki/\").load_data()\n",
    "index = MultiModalVectorStoreIndex.from_documents(\n",
    "    documents,\n",
    "    storage_context=storage_context,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "def plot_images(image_paths):\n",
    "    images_shown = 0\n",
    "    plt.figure(figsize=(16, 9))\n",
    "    for img_path in image_paths:\n",
    "        if os.path.isfile(img_path):\n",
    "            image = Image.open(img_path)\n",
    "\n",
    "            plt.subplot(2, 3, images_shown + 1)\n",
    "            plt.imshow(image)\n",
    "            plt.xticks([])\n",
    "            plt.yticks([])\n",
    "\n",
    "            images_shown += 1\n",
    "            if images_shown >= 9:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_query = \"what is the Labour Party?\"\n",
    "# generate  retrieval results\n",
    "retriever = index.as_retriever(similarity_top_k=3, image_similarity_top_k=5)\n",
    "retrieval_results = retriever.retrieve(test_query)\n",
    "\n",
    "retrieval_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.response.notebook_utils import display_source_node\n",
    "from llama_index.core.schema import ImageNode\n",
    "\n",
    "retrieved_image = []\n",
    "for res_node in retrieval_results:\n",
    "    if isinstance(res_node.node, ImageNode):\n",
    "        retrieved_image.append(res_node.node.metadata[\"file_path\"])\n",
    "    else:\n",
    "        display_source_node(res_node, source_length=200)\n",
    "\n",
    "plot_images(retrieved_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_query = \"Who created RoboCop?\"\n",
    "# generate  retrieval results\n",
    "retriever = index.as_retriever(similarity_top_k=3, image_similarity_top_k=5)\n",
    "retrieval_results = retriever.retrieve(test_query)\n",
    "\n",
    "retrieved_image = []\n",
    "for res_node in retrieval_results:\n",
    "    if isinstance(res_node.node, ImageNode):\n",
    "        retrieved_image.append(res_node.node.metadata[\"file_path\"])\n",
    "    else:\n",
    "        display_source_node(res_node, source_length=200)\n",
    "\n",
    "plot_images(retrieved_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_query = \"What does OpenAI do?\"\n",
    "# generate  retrieval results\n",
    "retriever = index.as_retriever(similarity_top_k=3, image_similarity_top_k=5)\n",
    "retrieval_results = retriever.retrieve(test_query)\n",
    "\n",
    "retrieved_image = []\n",
    "for res_node in retrieval_results:\n",
    "    if isinstance(res_node.node, ImageNode):\n",
    "        retrieved_image.append(res_node.node.metadata[\"file_path\"])\n",
    "    else:\n",
    "        display_source_node(res_node, source_length=200)\n",
    "\n",
    "plot_images(retrieved_image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
