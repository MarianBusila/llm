{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "from IPython.display import HTML, Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.api_core import retry\n",
    "\n",
    "\n",
    "is_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 503})\n",
    "\n",
    "genai.models.Models.generate_content = retry.Retry(\n",
    "    predicate=is_retriable)(genai.models.Models.generate_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load api keys\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, imagine you have a really smart dog. But this dog isn't just smart, it can learn new tricks by itself and even come up with new games to play!\n",
      "\n",
      "That's kind of like AI, or Artificial Intelligence.\n",
      "\n",
      "Think of it like this:\n",
      "\n",
      "*   **Artificial:**  That means it's not real, like a robot. It's made by people.\n",
      "*   **Intelligence:** That means it's smart, like you when you solve a puzzle!\n",
      "\n",
      "So, AI is a way to make computers really smart.  We give the computer lots and lots of information, like feeding a dog a big bowl of kibble. Then, the computer learns from that information and figures out how to do things on its own.\n",
      "\n",
      "For example:\n",
      "\n",
      "*   **AI can help you find your favorite cartoons on TV.** It knows which shows you like because you've watched them before.\n",
      "*   **AI can help you play games.** It can be a really good player because it has learned all the best moves.\n",
      "*   **AI can even help cars drive themselves!** It sees the road and the other cars and knows what to do.\n",
      "\n",
      "So, AI is like giving computers super-smart brains so they can help us with all sorts of things! It's a bit like magic, but it's made with computers and lots and lots of information.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "client = genai.Client(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    contents=\"Explain the AI to me like I am a kid.\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Okay, imagine you have a super smart puppy named Sparky. Sparky can learn to do tricks, right?\n",
       "\n",
       "AI is like teaching Sparky how to do even MORE tricks, but these tricks involve thinking and solving problems!\n",
       "\n",
       "Instead of just sitting or fetching, AI can learn to:\n",
       "\n",
       "*   **Recognize things:** Like seeing pictures of cats and dogs and knowing which is which, even if they look a little different! We can show the AI lots and lots of pictures, and it learns from them.\n",
       "*   **Talk:** AI can understand what you say and even talk back to you! It's like teaching Sparky to understand words and maybe even bark out some simple answers. (Think of Siri or Alexa, but maybe a little less perfect!)\n",
       "*   **Play games:** AI can learn to play games like checkers or chess and get really good at them! It learns from playing lots and lots of games and figuring out what moves work best.\n",
       "*   **Drive a car:** Some AI is being taught to drive cars! It learns to see the road, understand traffic lights, and avoid other cars. That way, humans can sit back and relax.\n",
       "\n",
       "**So, how does it work?**\n",
       "\n",
       "Think of it like giving Sparky lots and lots of examples.\n",
       "\n",
       "*   For recognizing cats, we show Sparky (the AI) thousands of cat pictures. It notices things like pointy ears, whiskers, and fluffy fur.\n",
       "*   For talking, we give Sparky lots of books and conversations to listen to. It learns how words are used together and what they mean.\n",
       "\n",
       "AI uses *computer programs* to do this learning. These programs are like the instructions you would use to teach Sparky tricks. The instructions tell the computer how to look for patterns and learn from examples.\n",
       "\n",
       "**But, AI is not really thinking like a human.**\n",
       "\n",
       "It's more like having a really, really good memory and being able to spot patterns really, really quickly. It can't feel emotions or have its own ideas, at least not yet.\n",
       "\n",
       "So, AI is like a super smart puppy learning amazing tricks, but it's still a program made by people to help us solve problems and do cool things.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Zlork! It's nice to meet you. How can I help you today?\n",
      "\n",
      "Why don't scientists trust atoms?\n",
      "\n",
      "Because they make up everything!\n",
      "\n",
      "Yes, I remember your name is Zlork.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chat = client.chats.create(model=\"gemini-2.0-flash\", history=[])\n",
    "response = chat.send_message(\"Hello! My name is Zlork.\")\n",
    "print(response.text)\n",
    "response = chat.send_message(\"Can you tell me a joke?\")\n",
    "print(response.text)\n",
    "response = chat.send_message(\"Do you remember my name?\")\n",
    "print(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/chat-bison-001 PaLM 2 Chat (Legacy)\n",
      "models/text-bison-001 PaLM 2 (Legacy)\n",
      "models/embedding-gecko-001 Embedding Gecko\n",
      "models/gemini-1.0-pro-vision-latest Gemini 1.0 Pro Vision\n",
      "models/gemini-pro-vision Gemini 1.0 Pro Vision\n",
      "models/gemini-1.5-pro-latest Gemini 1.5 Pro Latest\n",
      "models/gemini-1.5-pro-001 Gemini 1.5 Pro 001\n",
      "models/gemini-1.5-pro-002 Gemini 1.5 Pro 002\n",
      "models/gemini-1.5-pro Gemini 1.5 Pro\n",
      "models/gemini-1.5-flash-latest Gemini 1.5 Flash Latest\n",
      "models/gemini-1.5-flash-001 Gemini 1.5 Flash 001\n",
      "models/gemini-1.5-flash-001-tuning Gemini 1.5 Flash 001 Tuning\n",
      "models/gemini-1.5-flash Gemini 1.5 Flash\n",
      "models/gemini-1.5-flash-002 Gemini 1.5 Flash 002\n",
      "models/gemini-1.5-flash-8b Gemini 1.5 Flash-8B\n",
      "models/gemini-1.5-flash-8b-001 Gemini 1.5 Flash-8B 001\n",
      "models/gemini-1.5-flash-8b-latest Gemini 1.5 Flash-8B Latest\n",
      "models/gemini-1.5-flash-8b-exp-0827 Gemini 1.5 Flash 8B Experimental 0827\n",
      "models/gemini-1.5-flash-8b-exp-0924 Gemini 1.5 Flash 8B Experimental 0924\n",
      "models/gemini-2.5-pro-exp-03-25 Gemini 2.5 Pro Experimental 03-25\n",
      "models/gemini-2.5-pro-preview-03-25 Gemini 2.5 Pro Preview 03-25\n",
      "models/gemini-2.0-flash-exp Gemini 2.0 Flash Experimental\n",
      "models/gemini-2.0-flash Gemini 2.0 Flash\n",
      "models/gemini-2.0-flash-001 Gemini 2.0 Flash 001\n",
      "models/gemini-2.0-flash-exp-image-generation Gemini 2.0 Flash (Image Generation) Experimental\n",
      "models/gemini-2.0-flash-lite-001 Gemini 2.0 Flash-Lite 001\n",
      "models/gemini-2.0-flash-lite Gemini 2.0 Flash-Lite\n",
      "models/gemini-2.0-flash-lite-preview-02-05 Gemini 2.0 Flash-Lite Preview 02-05\n",
      "models/gemini-2.0-flash-lite-preview Gemini 2.0 Flash-Lite Preview\n",
      "models/gemini-2.0-pro-exp Gemini 2.0 Pro Experimental\n",
      "models/gemini-2.0-pro-exp-02-05 Gemini 2.0 Pro Experimental 02-05\n",
      "models/gemini-exp-1206 Gemini Experimental 1206\n",
      "models/gemini-2.0-flash-thinking-exp-01-21 Gemini 2.0 Flash Thinking Experimental 01-21\n",
      "models/gemini-2.0-flash-thinking-exp Gemini 2.0 Flash Thinking Experimental 01-21\n",
      "models/gemini-2.0-flash-thinking-exp-1219 Gemini 2.0 Flash Thinking Experimental\n",
      "models/learnlm-1.5-pro-experimental LearnLM 1.5 Pro Experimental\n",
      "models/gemma-3-1b-it Gemma 3 1B\n",
      "models/gemma-3-4b-it Gemma 3 4B\n",
      "models/gemma-3-12b-it Gemma 3 12B\n",
      "models/gemma-3-27b-it Gemma 3 27B\n",
      "models/embedding-001 Embedding 001\n",
      "models/text-embedding-004 Text Embedding 004\n",
      "models/gemini-embedding-exp-03-07 Gemini Embedding Experimental 03-07\n",
      "models/gemini-embedding-exp Gemini Embedding Experimental\n",
      "models/aqa Model that performs Attributed Question Answering.\n",
      "models/imagen-3.0-generate-002 Imagen 3.0 002 model\n"
     ]
    }
   ],
   "source": [
    "for model in client.models.list():\n",
    "    print(model.name, model.display_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'models/gemini-2.0-flash', 'display_name': 'Gemini 2.0 Flash', 'description': 'Gemini 2.0 Flash', 'version': '2.0', 'tuned_model_info': {}, 'input_token_limit': 1048576, 'output_token_limit': 8192, 'supported_actions': ['generateContent', 'countTokens']}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "for model in client.models.list():\n",
    "    if model.name == \"models/gemini-2.0-flash\":\n",
    "        print(model.to_json_dict())\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## The Humble Olive: A Cornerstone of Modern Society\n",
      "\n",
      "The olive, the fruit of the *Olea europaea* tree, has a history intertwined with civilization itself.  Beyond its biblical connotations of peace and prosperity, the olive and its derivatives are far more than a symbol; they are a foundational element of modern society, contributing to our diets, economies, health, and even our sense of cultural identity. While often overlooked amidst the complexities of contemporary life, the olive's importance is undeniable,\n"
     ]
    }
   ],
   "source": [
    "from google.genai import types\n",
    "\n",
    "short_config = types.GenerateContentConfig(max_output_tokens=100)\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    contents=\"Write a 1000 word essay on the importance of olives in modern society.\",\n",
    "    config=short_config)\n",
    "print(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lavender\n",
      "\n",
      "Mauve\n",
      "\n",
      "Fuchsia\n",
      "\n",
      "Fuchsia\n",
      "\n",
      "Lavender\n",
      "\n"
     ]
    }
   ],
   "source": [
    "high_temp_config = types.GenerateContentConfig(temperature=2.0)\n",
    "\n",
    "for _ in range(5):\n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-2.0-flash\",\n",
    "        contents=\"Pick a random colour. Respond in a single word.\",\n",
    "        config=high_temp_config)\n",
    "    print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Azure\n",
      "\n",
      "Azure\n",
      "\n",
      "Azure\n",
      "\n",
      "Azure\n",
      "\n",
      "Azure\n",
      "\n"
     ]
    }
   ],
   "source": [
    "low_temp_config = types.GenerateContentConfig(temperature=0.0)\n",
    "\n",
    "for _ in range(5):\n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-2.0-flash\",\n",
    "        contents=\"Pick a random colour. Respond in a single word.\",\n",
    "        config=low_temp_config)\n",
    "    print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Zero shot prompt\n",
    "model_config = types.GenerateContentConfig(temperature=0.1, top_p=1, max_output_tokens=5)\n",
    "zero_shot_prompt = \"\"\"\n",
    "Review: \"Her\" is a disturbing study revealing the direction\n",
    "humanity is headed if AI is allowed to keep evolving,\n",
    "unchecked. I wish there were more movies like this masterpiece.\n",
    "Sentiment: \n",
    "\"\"\"\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    contents=zero_shot_prompt,\n",
    "    config=model_config)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive\n",
      "Sentiment.POSITIVE <enum 'Sentiment'>\n"
     ]
    }
   ],
   "source": [
    "# enum mode\n",
    "import enum\n",
    "\n",
    "class Sentiment(enum.Enum):\n",
    "    POSITIVE = \"positive\"\n",
    "    NEGATIVE = \"negative\"\n",
    "    NEUTRAL = \"neutral\"\n",
    "    \n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    contents=zero_shot_prompt,\n",
    "    config=types.GenerateContentConfig(\n",
    "        response_mime_type=\"text/x.enum\",\n",
    "        response_schema=Sentiment\n",
    "    ))\n",
    "print(response.text)\n",
    "\n",
    "enum_response = response.parsed\n",
    "print(enum_response, type(enum_response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "    \"size\": \"medium\",\n",
      "    \"toppings\": [\"pineapple\", \"ham\"]\n",
      "}\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Few shot prompt\n",
    "few_shot_prompt = \"\"\" Parse a customer's pizza order into a valid JSON:\n",
    "\n",
    "EXAMPLE:\n",
    "I want a small pizza with cheese, tomato sauce and peperoni.\n",
    "JSON Response:\n",
    "```\n",
    "{\n",
    "    \"size\": \"small\",\n",
    "    \"toppings\": [\"cheese\", \"tomato sauce\", \"peperoni\"]\n",
    "}\n",
    "```\n",
    "\n",
    "EXAMPLE:\n",
    "Can I get a large pizza with mushrooms, olives and mozzarella?\n",
    "JSON Response:\n",
    "```\n",
    "{\n",
    "    \"size\": \"large\",\n",
    "    \"toppings\": [\"mushrooms\", \"olives\", \"mozzarella\"]\n",
    "}\n",
    "```\n",
    "\n",
    "ORDER:\n",
    "\"\"\"\n",
    "\n",
    "customer_order = \"I would like a medium pizza with pineapple and ham.\"\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    config=types.GenerateContentConfig(temperature=0.1, top_p=1, max_output_tokens=100),\n",
    "    contents=[few_shot_prompt, customer_order])\n",
    "print(response.text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"size\": \"large\",\n",
      "  \"ingredients\": [\"apple\", \"chocolate\"],\n",
      "  \"type\": \"dessert\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Json mode\n",
    "import typing_extensions as typing\n",
    "\n",
    "class PizzaOrder(typing.TypedDict):\n",
    "    size: str\n",
    "    ingredients: list[str]\n",
    "    type: str\n",
    "    \n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    config=types.GenerateContentConfig(\n",
    "        response_mime_type=\"application/json\",\n",
    "        response_schema=PizzaOrder\n",
    "    ),\n",
    "    contents=\"Can I have a large dessert pizza with apple and chocolate?\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here's how to solve this:\n",
       "\n",
       "1. **Find the age difference:** When you were 4, your partner was 3 * 4 = 12 years old.\n",
       "\n",
       "2. **Calculate the age difference:** The age difference between you and your partner is 12 - 4 = 8 years.\n",
       "\n",
       "3. **Determine the partner's current age:** Since the age difference remains constant, your partner is currently 20 + 8 = 28 years old.\n",
       "\n",
       "**Answer:** Your partner is 28 years old.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Chain of Thought Prompting\n",
    "prompt = \"\"\"When I was 4 years old, my partner was 3 times my age. Now,\n",
    "I am 20 years old. How old is my partner? Let's think step by step.\"\"\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model='gemini-2.0-flash',\n",
    "    contents=prompt)\n",
    "\n",
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thought 1\n",
      "I need to find the transformers NLP paper and then identify the youngest author listed on it.\n",
      "\n",
      "Action 1\n",
      "<search>transformers NLP paper</search>\n",
      "\n",
      "Thought 2\n",
      "The paper lists the authors: Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin. I need to find the youngest of these authors. I'll start by searching for Aidan N. Gomez since \"Aidan\" is often a younger-sounding name.\n",
      "\n",
      "Action 2\n",
      "<search>Aidan N. Gomez</search>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ReAct Prompting\n",
    "model_instructions = \"\"\"\n",
    "Solve a question answering task with interleaving Thought, Action, Observation steps. Thought can reason about the current situation,\n",
    "Observation is understanding relevant information from an Action's output and Action can be one of three types:\n",
    " (1) <search>entity</search>, which searches the exact entity on Wikipedia and returns the first paragraph if it exists. If not, it\n",
    "     will return some similar entities to search and you can try to search the information from those topics.\n",
    " (2) <lookup>keyword</lookup>, which returns the next sentence containing keyword in the current context. This only does exact matches,\n",
    "     so keep your searches short.\n",
    " (3) <finish>answer</finish>, which returns the answer and finishes the task.\n",
    "\"\"\"\n",
    "\n",
    "example1 = \"\"\"Question\n",
    "Musician and satirist Allie Goertz wrote a song about the \"The Simpsons\" character Milhouse, who Matt Groening named after who?\n",
    "\n",
    "Thought 1\n",
    "The question simplifies to \"The Simpsons\" character Milhouse is named after who. I only need to search Milhouse and find who it is named after.\n",
    "\n",
    "Action 1\n",
    "<search>Milhouse</search>\n",
    "\n",
    "Observation 1\n",
    "Milhouse Mussolini Van Houten is a recurring character in the Fox animated television series The Simpsons voiced by Pamela Hayden and created by Matt Groening.\n",
    "\n",
    "Thought 2\n",
    "The paragraph does not tell who Milhouse is named after, maybe I can look up \"named after\".\n",
    "\n",
    "Action 2\n",
    "<lookup>named after</lookup>\n",
    "\n",
    "Observation 2\n",
    "Milhouse was named after U.S. president Richard Nixon, whose middle name was Milhous.\n",
    "\n",
    "Thought 3\n",
    "Milhouse was named after U.S. president Richard Nixon, so the answer is Richard Nixon.\n",
    "\n",
    "Action 3\n",
    "<finish>Richard Nixon</finish>\n",
    "\"\"\"\n",
    "\n",
    "example2 = \"\"\"Question\n",
    "What is the elevation range for the area that the eastern sector of the Colorado orogeny extends into?\n",
    "\n",
    "Thought 1\n",
    "I need to search Colorado orogeny, find the area that the eastern sector of the Colorado orogeny extends into, then find the elevation range of the area.\n",
    "\n",
    "Action 1\n",
    "<search>Colorado orogeny</search>\n",
    "\n",
    "Observation 1\n",
    "The Colorado orogeny was an episode of mountain building (an orogeny) in Colorado and surrounding areas.\n",
    "\n",
    "Thought 2\n",
    "It does not mention the eastern sector. So I need to look up eastern sector.\n",
    "\n",
    "Action 2\n",
    "<lookup>eastern sector</lookup>\n",
    "\n",
    "Observation 2\n",
    "The eastern sector extends into the High Plains and is called the Central Plains orogeny.\n",
    "\n",
    "Thought 3\n",
    "The eastern sector of Colorado orogeny extends into the High Plains. So I need to search High Plains and find its elevation range.\n",
    "\n",
    "Action 3\n",
    "<search>High Plains</search>\n",
    "\n",
    "Observation 3\n",
    "High Plains refers to one of two distinct land regions\n",
    "\n",
    "Thought 4\n",
    "I need to instead search High Plains (United States).\n",
    "\n",
    "Action 4\n",
    "<search>High Plains (United States)</search>\n",
    "\n",
    "Observation 4\n",
    "The High Plains are a subregion of the Great Plains. From east to west, the High Plains rise in elevation from around 1,800 to 7,000 ft (550 to 2,130m).\n",
    "\n",
    "Thought 5\n",
    "High Plains rise in elevation from around 1,800 to 7,000 ft, so the answer is 1,800 to 7,000 ft.\n",
    "\n",
    "Action 5\n",
    "<finish>1,800 to 7,000 ft</finish>\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"Question\n",
    "Who was the youngest author listed on the transformers NLP paper?\n",
    "\"\"\"\n",
    "\n",
    "# You will perform the Action; so generate up to, but not including, the Observation.\n",
    "react_config = types.GenerateContentConfig(\n",
    "    stop_sequences=[\"\\nObservation\"],\n",
    "    system_instruction=model_instructions + example1 + example2,\n",
    ")\n",
    "\n",
    "# Create a chat that has the model instructions and examples pre-seeded.\n",
    "react_chat = client.chats.create(\n",
    "    model='gemini-2.0-flash',\n",
    "    config=react_config,\n",
    ")\n",
    "\n",
    "resp = react_chat.send_message(question)\n",
    "print(resp.text)\n",
    "\n",
    "observation = \"\"\"Observation 1\n",
    "[1706.03762] Attention Is All You Need\n",
    "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin\n",
    "We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.\n",
    "\"\"\"\n",
    "resp = react_chat.send_message(observation)\n",
    "print(resp.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The youngest author listed on the \"Attention is All You Need\" paper (which introduced the Transformer architecture and is often referred to as *the* \"transformers NLP paper\") is **Aidan N. Gomez**.\n",
       "\n",
       "While it's difficult to know their exact ages at the time of publication (2017) without access to birthdates, Aidan N. Gomez was generally considered the most junior and youngest author on the team. He was a PhD student at the University of Toronto at the time, while many of the other authors were more senior researchers at Google Brain.\n",
       "\n",
       "Therefore, based on career stage and general perception, **Aidan N. Gomez** is the most likely youngest author."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Thinking mode\n",
    "import io\n",
    "from IPython.display import Markdown, clear_output\n",
    "\n",
    "\n",
    "response = client.models.generate_content_stream(\n",
    "    model='gemini-2.0-flash-thinking-exp',\n",
    "    contents='Who was the youngest author listed on the transformers NLP paper?',\n",
    ")\n",
    "\n",
    "buf = io.StringIO()\n",
    "for chunk in response:\n",
    "    buf.write(chunk.text)\n",
    "    # Display the response as it is streamed\n",
    "    print(chunk.text, end='')\n",
    "\n",
    "# And then render the finished response as formatted markdown.\n",
    "clear_output()\n",
    "Markdown(buf.getvalue())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "def factorial(n):\n",
       "  \"\"\"\n",
       "  Calculates the factorial of a non-negative integer.\n",
       "\n",
       "  Args:\n",
       "    n: A non-negative integer.\n",
       "\n",
       "  Returns:\n",
       "    The factorial of n.\n",
       "  \"\"\"\n",
       "  if n == 0:\n",
       "    return 1\n",
       "  else:\n",
       "    return n * factorial(n-1)\n",
       "```\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Code generation\n",
    "code_prompt = \"\"\"\n",
    "Write a Python function to calculate the factorial of a number. No explanation, provide only the code.\n",
    "\"\"\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model='gemini-2.0-flash',\n",
    "    config=types.GenerateContentConfig(\n",
    "        temperature=1,\n",
    "        top_p=1,\n",
    "        max_output_tokens=1024,\n",
    "    ),\n",
    "    contents=code_prompt)\n",
    "\n",
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'Okay, I understand. First, I will generate the first 14 odd prime '\n",
      "         'numbers. Remember that a prime number is a number greater than 1 '\n",
      "         'that has only two factors: 1 and itself. Also, \"odd\" means it\\'s not '\n",
      "         'divisible by 2.\\n'\n",
      "         '\\n'\n",
      "         'Then I will sum them up.\\n'\n",
      "         '\\n'}\n",
      "-----------------------------\n",
      "{'executable_code': {'code': 'primes = []\\n'\n",
      "                             'number = 3\\n'\n",
      "                             'while len(primes) < 14:\\n'\n",
      "                             '    is_prime = True\\n'\n",
      "                             '    for i in range(2, int(number**0.5) + 1):\\n'\n",
      "                             '        if number % i == 0:\\n'\n",
      "                             '            is_prime = False\\n'\n",
      "                             '            break\\n'\n",
      "                             '    if is_prime:\\n'\n",
      "                             '        primes.append(number)\\n'\n",
      "                             '    number += 2\\n'\n",
      "                             '\\n'\n",
      "                             \"print(f'{primes=}')\\n\"\n",
      "                             '\\n'\n",
      "                             'import sympy\\n'\n",
      "                             'sum_of_primes = sum(primes)\\n'\n",
      "                             '\\n'\n",
      "                             \"print(f'{sum_of_primes=}')\\n\",\n",
      "                     'language': 'PYTHON'}}\n",
      "-----------------------------\n",
      "{'code_execution_result': {'outcome': 'OUTCOME_OK',\n",
      "                           'output': 'primes=[3, 5, 7, 11, 13, 17, 19, 23, 29, '\n",
      "                                     '31, 37, 41, 43, 47]\\n'\n",
      "                                     'sum_of_primes=326\\n'}}\n",
      "-----------------------------\n",
      "{'text': 'The first 14 odd prime numbers are 3, 5, 7, 11, 13, 17, 19, 23, 29, '\n",
      "         '31, 37, 41, 43, and 47. Their sum is 326.\\n'}\n",
      "-----------------------------\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Okay, I understand. First, I will generate the first 14 odd prime numbers. Remember that a prime number is a number greater than 1 that has only two factors: 1 and itself. Also, \"odd\" means it's not divisible by 2.\n",
       "\n",
       "Then I will sum them up.\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "primes = []\n",
       "number = 3\n",
       "while len(primes) < 14:\n",
       "    is_prime = True\n",
       "    for i in range(2, int(number**0.5) + 1):\n",
       "        if number % i == 0:\n",
       "            is_prime = False\n",
       "            break\n",
       "    if is_prime:\n",
       "        primes.append(number)\n",
       "    number += 2\n",
       "\n",
       "print(f'{primes=}')\n",
       "\n",
       "import sympy\n",
       "sum_of_primes = sum(primes)\n",
       "\n",
       "print(f'{sum_of_primes=}')\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "primes=[3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47]\n",
       "sum_of_primes=326\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The first 14 odd prime numbers are 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, and 47. Their sum is 326.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Code execution\n",
    "from pprint import pprint\n",
    "config = types.GenerateContentConfig(tools=[types.Tool(code_execution=types.ToolCodeExecution())])\n",
    "code_exec_prompt = \"\"\"\n",
    "Generate the first 14 odd prime numbers, then calculate their sum.\n",
    "\"\"\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model='gemini-2.0-flash',\n",
    "    config=config,\n",
    "    contents=code_exec_prompt)\n",
    "\n",
    "for part in response.candidates[0].content.parts:\n",
    "    pprint(part.to_json_dict())\n",
    "    print(\"-----------------------------\")\n",
    "    \n",
    "for part in response.candidates[0].content.parts:\n",
    "    if part.text:\n",
    "        display(Markdown(part.text))\n",
    "    elif part.executable_code:\n",
    "        display(Markdown(f'```python\\n{part.executable_code.code}\\n```'))\n",
    "    elif part.code_execution_result:\n",
    "        if part.code_execution_result.outcome != 'OUTCOME_OK':\n",
    "            display(Markdown(f'## Status {part.code_execution_result.outcome}'))\n",
    "\n",
    "        display(Markdown(f'```\\n{part.code_execution_result.output}\\n```'))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "This file, `git-prompt.sh`, is a shell script designed to enhance your command-line prompt with information about the current Git repository.\n",
       "\n",
       "**What it does:**\n",
       "\n",
       "At a high level, this script aims to:\n",
       "\n",
       "*   **Display Git status information in your prompt:**  It shows the current branch, whether there are uncommitted changes, the status of tracked and untracked files, and other relevant Git information.\n",
       "*   **Customize the prompt's appearance:**  It allows you to configure colors, symbols, and the overall layout of the Git-related part of your prompt.\n",
       "*   **Provide asynchronous updates:**  It fetches remote Git information in the background so that your prompt updates efficiently without blocking your terminal.\n",
       "*   **Support various shell environments:** It tries to be compatible with both Bash and Zsh shells.\n",
       "*   **Support Virtual Environments:** Displays the current Python Virtual Environment, if activated\n",
       "\n",
       "**Why you would use it:**\n",
       "\n",
       "You'd use this script if you want a more informative and visually appealing command-line prompt when working with Git repositories. The benefits include:\n",
       "\n",
       "*   **Quickly see the Git status:**  You can instantly see if you have any changes to commit, if you're on the correct branch, or if your local branch is out of sync with the remote.\n",
       "*   **Improved workflow:** The additional context helps you stay organized and avoid common Git-related mistakes.\n",
       "*   **Customization:** You can tailor the prompt to your specific preferences and needs.\n",
       "*   **Efficiency:** The asynchronous updates ensure that the prompt stays responsive even when the Git repository is large or the network connection is slow.\n",
       "\n",
       "In short, `git-prompt.sh` is a tool to make your command-line experience when using Git more productive and enjoyable by providing real-time Git status in your prompt. You would typically source this file in your `.bashrc` or `.zshrc` file to enable it.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Explaining code\n",
    "file_contents = !curl https://raw.githubusercontent.com/magicmonty/bash-git-prompt/refs/heads/master/gitprompt.sh\n",
    "\n",
    "explain_prompt = f\"\"\"\n",
    "Please explain what this file does at a very high level. What is it, and why would I use it?\n",
    "\n",
    "```\n",
    "{file_contents}\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model='gemini-2.0-flash',\n",
    "    contents=explain_prompt)\n",
    "\n",
    "Markdown(response.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
