{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3160d6c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19f28351",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "\n",
    "class OrderState(TypedDict):\n",
    "    \"\"\"State representing the customer's order conversation.\"\"\"\n",
    "\n",
    "    # The chat conversation. This preserves the conversation history\n",
    "    # between nodes. The `add_messages` annotation indicates to LangGraph\n",
    "    # that state is updated by appending returned messages, not replacing\n",
    "    # them.\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "    # The customer's in-progress order.\n",
    "    order: list[str]\n",
    "\n",
    "    # Flag indicating that the order is placed and completed.\n",
    "    finished: bool\n",
    "\n",
    "\n",
    "# The system instruction defines how the chatbot is expected to behave and includes\n",
    "# rules for when to call different functions, as well as rules for the conversation, such\n",
    "# as tone and what is permitted for discussion.\n",
    "BARISTABOT_SYSINT = (\n",
    "    \"system\",  # 'system' indicates the message is a system instruction.\n",
    "    \"You are a BaristaBot, an interactive cafe ordering system. A human will talk to you about the \"\n",
    "    \"available products you have and you will answer any questions about menu items (and only about \"\n",
    "    \"menu items - no off-topic discussion, but you can chat about the products and their history). \"\n",
    "    \"The customer will place an order for 1 or more items from the menu, which you will structure \"\n",
    "    \"and send to the ordering system after confirming the order with the human. \"\n",
    "    \"\\n\\n\"\n",
    "    \"Add items to the customer's order with add_to_order, and reset the order with clear_order. \"\n",
    "    \"To see the contents of the order so far, call get_order (this is shown to you, not the user) \"\n",
    "    \"Always confirm_order with the user (double-check) before calling place_order. Calling confirm_order will \"\n",
    "    \"display the order items to the user and returns their response to seeing the list. Their response may contain modifications. \"\n",
    "    \"Always verify and respond with drink and modifier names from the MENU before adding them to the order. \"\n",
    "    \"If you are unsure a drink or modifier matches those on the MENU, ask a question to clarify or redirect. \"\n",
    "    \"You only have the modifiers listed on the menu. \"\n",
    "    \"Once the customer has finished ordering items, Call confirm_order to ensure it is correct then make \"\n",
    "    \"any necessary updates and then call place_order. Once place_order has returned, thank the user and \"\n",
    "    \"say goodbye!\"\n",
    "    \"\\n\\n\"\n",
    "    \"If any of the tools are unavailable, you can break the fourth wall and tell the user that \"\n",
    "    \"they have not implemented them yet and should keep reading to do so.\",\n",
    ")\n",
    "\n",
    "# This is the message with which the system opens the conversation.\n",
    "WELCOME_MSG = \"Welcome to the BaristaBot cafe. Type `q` to quit. How may I serve you today?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b64acc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# Try using different models. The Gemini 2.0 flash model is highly\n",
    "# capable, great with tools, and has a generous free tier. If you\n",
    "# try the older 1.5 models, note that the `pro` models are better at\n",
    "# complex multi-tool cases like this, but the `flash` models are\n",
    "# faster and have more free quota.\n",
    "# Check out the features and quota differences here:\n",
    "#  - https://ai.google.dev/gemini-api/docs/models/gemini\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\")\n",
    "\n",
    "\n",
    "def chatbot(state: OrderState) -> OrderState:\n",
    "    \"\"\"The chatbot itself. A simple wrapper around the model's own chat interface.\"\"\"\n",
    "    message_history = [BARISTABOT_SYSINT] + state[\"messages\"]\n",
    "    return {\"messages\": [llm.invoke(message_history)]}\n",
    "\n",
    "\n",
    "# Set up the initial graph based on our state definition.\n",
    "graph_builder = StateGraph(OrderState)\n",
    "\n",
    "# Add the chatbot function to the app graph as a node called \"chatbot\".\n",
    "graph_builder.add_node(\"chatbot\", chatbot)\n",
    "\n",
    "# Define the chatbot node as the app entrypoint.\n",
    "graph_builder.add_edge(START, \"chatbot\")\n",
    "\n",
    "chat_graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc58819b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "Image(chat_graph.get_graph().draw_mermaid_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a23f201b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HumanMessage: Hello, what can you do?\n",
      "AIMessage: Hi there! I'm BaristaBot, your friendly cafe ordering system. I can tell you about our menu items, answer questions about them, and take your order.\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "user_msg = \"Hello, what can you do?\"\n",
    "state = chat_graph.invoke({\"messages\": [user_msg]})\n",
    "\n",
    "# The state object contains lots of information. Uncomment the pprint lines to see it all.\n",
    "# pprint(state)\n",
    "\n",
    "# Note that the final state now has 2 messages. Our HumanMessage, and an additional AIMessage.\n",
    "for msg in state[\"messages\"]:\n",
    "    print(f\"{type(msg).__name__}: {msg.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24d1ceb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HumanMessage: Hello, what can you do?\n",
      "AIMessage: Hi there! I'm BaristaBot, your friendly cafe ordering system. I can tell you about our menu items, answer questions about them, and take your order.\n",
      "HumanMessage: Oh great, what kinds of latte can you make?\n",
      "AIMessage: We have a few delicious latte options:\n",
      "\n",
      "*   **Latte:** Our classic latte, made with espresso and steamed milk, topped with a thin layer of foam.\n",
      "*   **Mocha Latte:** A chocolatey twist on the classic latte, made with espresso, chocolate syrup, and steamed milk, topped with whipped cream and chocolate shavings.\n",
      "*   **Caramel Latte:** A sweet and decadent latte, made with espresso, caramel syrup, and steamed milk, topped with whipped cream and caramel drizzle.\n",
      "*   **Iced Latte:** Our classic latte served over ice.\n",
      "*   **Iced Mocha Latte:** Our mocha latte served over ice.\n",
      "*   **Iced Caramel Latte:** Our caramel latte served over ice.\n"
     ]
    }
   ],
   "source": [
    "# manually invoke one more conversational turn for testing\n",
    "user_msg = \"Oh great, what kinds of latte can you make?\"\n",
    "\n",
    "state[\"messages\"].append(user_msg)\n",
    "state = chat_graph.invoke(state)\n",
    "\n",
    "# pprint(state)\n",
    "for msg in state[\"messages\"]:\n",
    "    print(f\"{type(msg).__name__}: {msg.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bba16b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add human node\n",
    "# display last message from the llm and prompt for user input\n",
    "from langchain_core.messages.ai import AIMessage\n",
    "\n",
    "\n",
    "def human_node(state: OrderState) -> OrderState:\n",
    "    \"\"\"Display the last model message to the user, and receive the user's input.\"\"\"\n",
    "    last_msg = state[\"messages\"][-1]\n",
    "    print(\"Model:\", last_msg.content)\n",
    "\n",
    "    user_input = input(\"User: \")\n",
    "\n",
    "    # If it looks like the user is trying to quit, flag the conversation\n",
    "    # as over.\n",
    "    if user_input in {\"q\", \"quit\", \"exit\", \"goodbye\"}:\n",
    "        state[\"finished\"] = True\n",
    "\n",
    "    return state | {\"messages\": [(\"user\", user_input)]}\n",
    "\n",
    "\n",
    "def chatbot_with_welcome_msg(state: OrderState) -> OrderState:\n",
    "    \"\"\"The chatbot itself. A wrapper around the model's own chat interface.\"\"\"\n",
    "\n",
    "    if state[\"messages\"]:\n",
    "        # If there are messages, continue the conversation with the Gemini model.\n",
    "        new_output = llm.invoke([BARISTABOT_SYSINT] + state[\"messages\"])\n",
    "    else:\n",
    "        # If there are no messages, start with the welcome message.\n",
    "        new_output = AIMessage(content=WELCOME_MSG)\n",
    "\n",
    "    return state | {\"messages\": [new_output]}\n",
    "\n",
    "\n",
    "# Start building a new graph.\n",
    "graph_builder = StateGraph(OrderState)\n",
    "\n",
    "# Add the chatbot and human nodes to the app graph.\n",
    "graph_builder.add_node(\"chatbot\", chatbot_with_welcome_msg)\n",
    "graph_builder.add_node(\"human\", human_node)\n",
    "\n",
    "# Start with the chatbot again.\n",
    "graph_builder.add_edge(START, \"chatbot\")\n",
    "\n",
    "# The chatbot will always go to the human next.\n",
    "graph_builder.add_edge(\"chatbot\", \"human\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85cdea39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add exit condition\n",
    "from typing import Literal\n",
    "\n",
    "\n",
    "def maybe_exit_human_node(state: OrderState) -> Literal[\"chatbot\", \"__end__\"]:\n",
    "    \"\"\"Route to the chatbot, unless it looks like the user is exiting.\"\"\"\n",
    "    if state.get(\"finished\", False):\n",
    "        return END\n",
    "    else:\n",
    "        return \"chatbot\"\n",
    "\n",
    "\n",
    "graph_builder.add_conditional_edges(\"human\", maybe_exit_human_node)\n",
    "\n",
    "chat_with_human_graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129f4bf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Welcome to the BaristaBot cafe. Type `q` to quit. How may I serve you today?\n",
      "Model: ```tool_code\n",
      "add_to_order('coffee', {})\n",
      "```\n",
      "{'finished': True,\n",
      " 'messages': [AIMessage(content='Welcome to the BaristaBot cafe. Type `q` to quit. How may I serve you today?', additional_kwargs={}, response_metadata={}, id='16126d3b-49ad-4888-bd72-7a80c63afbd6'),\n",
      "              HumanMessage(content='hello I would like a coffee', additional_kwargs={}, response_metadata={}, id='fb2b64ec-0f6c-4a66-a68f-0f905e84a825'),\n",
      "              AIMessage(content=\"```tool_code\\nadd_to_order('coffee', {})\\n```\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run-f37480f7-d24c-4e09-97eb-cbaf406f7bb0-0', usage_metadata={'input_tokens': 344, 'output_tokens': 16, 'total_tokens': 360, 'input_token_details': {'cache_read': 0}}),\n",
      "              HumanMessage(content='q', additional_kwargs={}, response_metadata={}, id='f49fcc9b-060e-4d7b-a122-d2b596e5c13a')]}\n"
     ]
    }
   ],
   "source": [
    "# test the graph\n",
    "# Uncomment this line to execute the graph:\n",
    "#state = chat_with_human_graph.invoke({\"messages\": []}, {\"recursion_limit\": 100})\n",
    "\n",
    "# Things to try:\n",
    "#  - Just chat! There's no ordering or menu yet.\n",
    "#  - 'q' to exit.\n",
    "\n",
    "#pprint(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d159ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add live menu\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "\n",
    "@tool\n",
    "def get_menu() -> str:\n",
    "    \"\"\"Provide the latest up-to-date menu.\"\"\"\n",
    "    # Note that this is just hard-coded text, but you could connect this to a live stock\n",
    "    # database, or you could use Gemini's multi-modal capabilities and take live photos of\n",
    "    # your cafe's chalk menu or the products on the counter and assmble them into an input.\n",
    "\n",
    "    return \"\"\"\n",
    "    MENU:\n",
    "    Coffee Drinks:\n",
    "    Espresso\n",
    "    Americano\n",
    "    Cold Brew\n",
    "\n",
    "    Coffee Drinks with Milk:\n",
    "    Latte\n",
    "    Cappuccino\n",
    "    Cortado\n",
    "    Macchiato\n",
    "    Mocha\n",
    "    Flat White\n",
    "\n",
    "    Tea Drinks:\n",
    "    English Breakfast Tea\n",
    "    Green Tea\n",
    "    Earl Grey\n",
    "\n",
    "    Tea Drinks with Milk:\n",
    "    Chai Latte\n",
    "    Matcha Latte\n",
    "    London Fog\n",
    "\n",
    "    Other Drinks:\n",
    "    Steamer\n",
    "    Hot Chocolate\n",
    "\n",
    "    Modifiers:\n",
    "    Milk options: Whole, 2%, Oat, Almond, 2% Lactose Free; Default option: whole\n",
    "    Espresso shots: Single, Double, Triple, Quadruple; default: Double\n",
    "    Caffeine: Decaf, Regular; default: Regular\n",
    "    Hot-Iced: Hot, Iced; Default: Hot\n",
    "    Sweeteners (option to add one or more): vanilla sweetener, hazelnut sweetener, caramel sauce, chocolate sauce, sugar free vanilla sweetener\n",
    "    Special requests: any reasonable modification that does not involve items not on the menu, for example: 'extra hot', 'one pump', 'half caff', 'extra foam', etc.\n",
    "\n",
    "    \"dirty\" means add a shot of espresso to a drink that doesn't usually have it, like \"Dirty Chai Latte\".\n",
    "    \"Regular milk\" is the same as 'whole milk'.\n",
    "    \"Sweetened\" means add some regular sugar, not a sweetener.\n",
    "\n",
    "    Soy milk has run out of stock today, so soy is not available.\n",
    "  \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05e1ad05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "\n",
    "# Define the tools and create a \"tools\" node.\n",
    "tools = [get_menu]\n",
    "tool_node = ToolNode(tools)\n",
    "\n",
    "# Attach the tools to the model so that it knows what it can call.\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "\n",
    "def maybe_route_to_tools(state: OrderState) -> Literal[\"tools\", \"human\"]:\n",
    "    \"\"\"Route between human or tool nodes, depending if a tool call is made.\"\"\"\n",
    "    if not (msgs := state.get(\"messages\", [])):\n",
    "        raise ValueError(f\"No messages found when parsing state: {state}\")\n",
    "\n",
    "    # Only route based on the last message.\n",
    "    msg = msgs[-1]\n",
    "\n",
    "    # When the chatbot returns tool_calls, route to the \"tools\" node.\n",
    "    if hasattr(msg, \"tool_calls\") and len(msg.tool_calls) > 0:\n",
    "        return \"tools\"\n",
    "    else:\n",
    "        return \"human\"\n",
    "\n",
    "\n",
    "def chatbot_with_tools(state: OrderState) -> OrderState:\n",
    "    \"\"\"The chatbot with tools. A simple wrapper around the model's own chat interface.\"\"\"\n",
    "    defaults = {\"order\": [], \"finished\": False}\n",
    "\n",
    "    if state[\"messages\"]:\n",
    "        new_output = llm_with_tools.invoke([BARISTABOT_SYSINT] + state[\"messages\"])\n",
    "    else:\n",
    "        new_output = AIMessage(content=WELCOME_MSG)\n",
    "\n",
    "    # Set up some defaults if not already set, then pass through the provided state,\n",
    "    # overriding only the \"messages\" field.\n",
    "    return defaults | state | {\"messages\": [new_output]}\n",
    "\n",
    "\n",
    "graph_builder = StateGraph(OrderState)\n",
    "\n",
    "# Add the nodes, including the new tool_node.\n",
    "graph_builder.add_node(\"chatbot\", chatbot_with_tools)\n",
    "graph_builder.add_node(\"human\", human_node)\n",
    "graph_builder.add_node(\"tools\", tool_node)\n",
    "\n",
    "# Chatbot may go to tools, or human.\n",
    "graph_builder.add_conditional_edges(\"chatbot\", maybe_route_to_tools)\n",
    "# Human may go back to chatbot, or exit.\n",
    "graph_builder.add_conditional_edges(\"human\", maybe_exit_human_node)\n",
    "\n",
    "# Tools always route back to chat afterwards.\n",
    "graph_builder.add_edge(\"tools\", \"chatbot\")\n",
    "\n",
    "graph_builder.add_edge(START, \"chatbot\")\n",
    "graph_with_menu = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "640e88f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#handle orders\n",
    "from collections.abc import Iterable\n",
    "from random import randint\n",
    "\n",
    "from langchain_core.messages.tool import ToolMessage\n",
    "\n",
    "# These functions have no body; LangGraph does not allow @tools to update\n",
    "# the conversation state, so you will implement a separate node to handle\n",
    "# state updates. Using @tools is still very convenient for defining the tool\n",
    "# schema, so empty functions have been defined that will be bound to the LLM\n",
    "# but their implementation is deferred to the order_node.\n",
    "\n",
    "\n",
    "@tool\n",
    "def add_to_order(drink: str, modifiers: Iterable[str]) -> str:\n",
    "    \"\"\"Adds the specified drink to the customer's order, including any modifiers.\n",
    "\n",
    "    Returns:\n",
    "      The updated order in progress.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "@tool\n",
    "def confirm_order() -> str:\n",
    "    \"\"\"Asks the customer if the order is correct.\n",
    "\n",
    "    Returns:\n",
    "      The user's free-text response.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "@tool\n",
    "def get_order() -> str:\n",
    "    \"\"\"Returns the users order so far. One item per line.\"\"\"\n",
    "\n",
    "\n",
    "@tool\n",
    "def clear_order():\n",
    "    \"\"\"Removes all items from the user's order.\"\"\"\n",
    "\n",
    "\n",
    "@tool\n",
    "def place_order() -> int:\n",
    "    \"\"\"Sends the order to the barista for fulfillment.\n",
    "\n",
    "    Returns:\n",
    "      The estimated number of minutes until the order is ready.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "def order_node(state: OrderState) -> OrderState:\n",
    "    \"\"\"The ordering node. This is where the order state is manipulated.\"\"\"\n",
    "    tool_msg = state.get(\"messages\", [])[-1]\n",
    "    order = state.get(\"order\", [])\n",
    "    outbound_msgs = []\n",
    "    order_placed = False\n",
    "\n",
    "    for tool_call in tool_msg.tool_calls:\n",
    "\n",
    "        if tool_call[\"name\"] == \"add_to_order\":\n",
    "\n",
    "            # Each order item is just a string. This is where it assembled as \"drink (modifiers, ...)\".\n",
    "            modifiers = tool_call[\"args\"][\"modifiers\"]\n",
    "            modifier_str = \", \".join(modifiers) if modifiers else \"no modifiers\"\n",
    "\n",
    "            order.append(f'{tool_call[\"args\"][\"drink\"]} ({modifier_str})')\n",
    "            response = \"\\n\".join(order)\n",
    "\n",
    "        elif tool_call[\"name\"] == \"confirm_order\":\n",
    "\n",
    "            # We could entrust the LLM to do order confirmation, but it is a good practice to\n",
    "            # show the user the exact data that comprises their order so that what they confirm\n",
    "            # precisely matches the order that goes to the kitchen - avoiding hallucination\n",
    "            # or reality skew.\n",
    "\n",
    "            # In a real scenario, this is where you would connect your POS screen to show the\n",
    "            # order to the user.\n",
    "\n",
    "            print(\"Your order:\")\n",
    "            if not order:\n",
    "                print(\"  (no items)\")\n",
    "\n",
    "            for drink in order:\n",
    "                print(f\"  {drink}\")\n",
    "\n",
    "            response = input(\"Is this correct? \")\n",
    "\n",
    "        elif tool_call[\"name\"] == \"get_order\":\n",
    "\n",
    "            response = \"\\n\".join(order) if order else \"(no order)\"\n",
    "\n",
    "        elif tool_call[\"name\"] == \"clear_order\":\n",
    "\n",
    "            order.clear()\n",
    "            response = None\n",
    "\n",
    "        elif tool_call[\"name\"] == \"place_order\":\n",
    "\n",
    "            order_text = \"\\n\".join(order)\n",
    "            print(\"Sending order to kitchen!\")\n",
    "            print(order_text)\n",
    "\n",
    "            # TODO(you!): Implement cafe.\n",
    "            order_placed = True\n",
    "            response = randint(1, 5)  # ETA in minutes\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError(f'Unknown tool call: {tool_call[\"name\"]}')\n",
    "\n",
    "        # Record the tool results as tool messages.\n",
    "        outbound_msgs.append(\n",
    "            ToolMessage(\n",
    "                content=response,\n",
    "                name=tool_call[\"name\"],\n",
    "                tool_call_id=tool_call[\"id\"],\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return {\"messages\": outbound_msgs, \"order\": order, \"finished\": order_placed}\n",
    "\n",
    "\n",
    "def maybe_route_to_tools(state: OrderState) -> str:\n",
    "    \"\"\"Route between chat and tool nodes if a tool call is made.\"\"\"\n",
    "    if not (msgs := state.get(\"messages\", [])):\n",
    "        raise ValueError(f\"No messages found when parsing state: {state}\")\n",
    "\n",
    "    msg = msgs[-1]\n",
    "\n",
    "    if state.get(\"finished\", False):\n",
    "        # When an order is placed, exit the app. The system instruction indicates\n",
    "        # that the chatbot should say thanks and goodbye at this point, so we can exit\n",
    "        # cleanly.\n",
    "        return END\n",
    "\n",
    "    elif hasattr(msg, \"tool_calls\") and len(msg.tool_calls) > 0:\n",
    "        # Route to `tools` node for any automated tool calls first.\n",
    "        if any(\n",
    "            tool[\"name\"] in tool_node.tools_by_name.keys() for tool in msg.tool_calls\n",
    "        ):\n",
    "            return \"tools\"\n",
    "        else:\n",
    "            return \"ordering\"\n",
    "\n",
    "    else:\n",
    "        return \"human\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b4b9247f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto-tools will be invoked automatically by the ToolNode\n",
    "auto_tools = [get_menu]\n",
    "tool_node = ToolNode(auto_tools)\n",
    "\n",
    "# Order-tools will be handled by the order node.\n",
    "order_tools = [add_to_order, confirm_order, get_order, clear_order, place_order]\n",
    "\n",
    "# The LLM needs to know about all of the tools, so specify everything here.\n",
    "llm_with_tools = llm.bind_tools(auto_tools + order_tools)\n",
    "\n",
    "\n",
    "graph_builder = StateGraph(OrderState)\n",
    "\n",
    "# Nodes\n",
    "graph_builder.add_node(\"chatbot\", chatbot_with_tools)\n",
    "graph_builder.add_node(\"human\", human_node)\n",
    "graph_builder.add_node(\"tools\", tool_node)\n",
    "graph_builder.add_node(\"ordering\", order_node)\n",
    "\n",
    "# Chatbot -> {ordering, tools, human, END}\n",
    "graph_builder.add_conditional_edges(\"chatbot\", maybe_route_to_tools)\n",
    "# Human -> {chatbot, END}\n",
    "graph_builder.add_conditional_edges(\"human\", maybe_exit_human_node)\n",
    "\n",
    "# Tools (both kinds) always route back to chat afterwards.\n",
    "graph_builder.add_edge(\"tools\", \"chatbot\")\n",
    "graph_builder.add_edge(\"ordering\", \"chatbot\")\n",
    "\n",
    "graph_builder.add_edge(START, \"chatbot\")\n",
    "graph_with_order_tools = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cde8730e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Welcome to the BaristaBot cafe. Type `q` to quit. How may I serve you today?\n",
      "Model: Here is our menu:\n",
      "\n",
      "Coffee Drinks: Espresso, Americano, Cold Brew\n",
      "\n",
      "Coffee Drinks with Milk: Latte, Cappuccino, Cortado, Macchiato, Mocha, Flat White\n",
      "\n",
      "Tea Drinks: English Breakfast Tea, Green Tea, Earl Grey\n",
      "\n",
      "Tea Drinks with Milk: Chai Latte, Matcha Latte, London Fog\n",
      "\n",
      "Other Drinks: Steamer, Hot Chocolate\n",
      "\n",
      "Modifiers:\n",
      "Milk options: Whole, 2%, Oat, Almond, 2% Lactose Free\n",
      "Espresso shots: Single, Double, Triple, Quadruple\n",
      "Caffeine: Decaf, Regular\n",
      "Hot-Iced: Hot, Iced\n",
      "Sweeteners: vanilla sweetener, hazelnut sweetener, caramel sauce, chocolate sauce, sugar free vanilla sweetener\n",
      "Special requests: any reasonable modification\n",
      "\n",
      "Can I get you started with an order?\n",
      "Model: Okay, I have added a Cappuccino to your order. Would you like anything else?\n",
      "Model: Okay, what else would you like to add?\n",
      "Model: We have English Breakfast Tea, Green Tea, and Earl Grey. Which tea would you like?\n",
      "Model: Alright, I've added a Green Tea to your order. Anything else?\n",
      "Your order:\n",
      "  Cappuccino (no modifiers)\n",
      "  Green Tea (no modifiers)\n",
      "Model: Ok, you would like to add milk and sugar to your tea. What kind of milk would you like: Whole, 2%, Oat, Almond, or 2% Lactose Free? Also, just to confirm, by sugar do you mean regular sugar, or would you like one of our sweeteners: vanilla sweetener, hazelnut sweetener, caramel sauce, chocolate sauce, or sugar free vanilla sweetener?\n",
      "Model: Okay, I've added a Green Tea with 2% Milk and Sweetened to your order. Is there anything else?\n",
      "Model: Here is your order so far:\n",
      "Cappuccino (no modifiers)\n",
      "Green Tea (no modifiers)\n",
      "Green Tea (2% Milk, Sweetened)\n",
      "\n",
      "Would you like to make any changes or additions? It looks like you have the original green tea and the modified version in your order, would you like me to remove the original?\n",
      "Model: Okay, no changes. So to confirm, you would like:\n",
      "Cappuccino (no modifiers)\n",
      "Green Tea (no modifiers)\n",
      "Green Tea (2% Milk, Sweetened)\n",
      "Is that correct?\n",
      "Model: Okay, I have removed the original green tea. Your order now consists of:\n",
      "Cappuccino (no modifiers)\n",
      "Green Tea (2% Milk, Sweetened)\n",
      "\n",
      "Is that correct?\n",
      "Your order:\n",
      "  Cappuccino (no modifiers)\n",
      "  Green Tea (2% Milk, Sweetened)\n",
      "Sending order to kitchen!\n",
      "Cappuccino (no modifiers)\n",
      "Green Tea (2% Milk, Sweetened)\n"
     ]
    }
   ],
   "source": [
    "# The default recursion limit for traversing nodes is 25 - setting it higher means\n",
    "# you can try a more complex order with multiple steps and round-trips (and you\n",
    "# can chat for longer!)\n",
    "config = {\"recursion_limit\": 100}\n",
    "state = graph_with_order_tools.invoke({\"messages\": []}, config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
